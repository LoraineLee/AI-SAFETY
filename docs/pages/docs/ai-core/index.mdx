---
title: AI Core (experimental)
layout:
  toc: false
---

# AI Core (experimental)

The Vercel AI SDK offers a unified way of calling large language models (LLMs) that can be used with any compatible provider:

- `generateText`: Generate text and call tools [ [API](/docs/ai-core/generate-text) ]
- `streamText`: Stream text and call tools
- `generateObject`: Generate a structured object that matches a schema
- `streamObject`: Stream a structured object that matches a schema

The AI functions share the same [prompt structure](/docs/ai-core/prompt) and the same [common settings](/docs/ai-core/settings).
The model is created using a language model provider, e.g. the [OpenAI provider](/docs/ai-core/openai) .
Here is a simple example for `generateText`:

```ts
import { experimental_generateText } from 'ai';
import { openai } from 'ai/openai';

const { text } = await experimental_generateText({
  model: openai.chat('gpt-3.5-turbo'),
  prompt: 'Invent a new holiday and describe its traditions.',
});
```

## Language Model Interface

Providers need to provide an implementation of the language model interface to be compatible with the AI SDK.
The AI SDK contains the following providers:

- [OpenAI Provider](/docs/ai-core/openai) (`ai/openai`)
- Mistral Provider (`ai/mistral`)

![AI SDK Diagram](/images/ai-sdk-diagram.png)
