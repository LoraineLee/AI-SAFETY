diff --git a/docs/pages/docs/api-reference/inkeep-stream.mdx b/docs/pages/docs/api-reference/inkeep-stream.mdx
new file mode 100644
index 0000000..1b4e0ad
--- /dev/null
+++ b/docs/pages/docs/api-reference/inkeep-stream.mdx
@@ -0,0 +1,245 @@
+# InkeepStream
+
+## `InkeepStream(res: Response, cb?: AIStreamCallbacks): ReadableStream` [#InkeepStream]
+
+The `InkeepStream` function is a utility that transforms the output from [Inkeep's](https://www.inkeep.com) API into a (ReadableStream)[(ReadableStream)[https://developer.mozilla.org/docs/Web/API/ReadableStream]]. It uses [AIStream](/docs/api-reference/ai-stream) under the hood, applying a specific parser for the Inkeep's response data structure.
+
+This works with the official Inkeep API, and it's supported in both Node.js, the [Edge Runtime](https://edge-runtime.vercel.app), and browser environments.
+
+## Parameters
+
+### `res: Response`
+
+The `Response` object returned by the request to the Inkeep API.
+
+### `cb?: InkeepAIStreamCallbacksAndOptions`
+
+This optional parameter can be an object containing the callback functions to handle the start, each token, completion, and other events of the AI response. In the absence of this parameter, default behavior is implemented.
+
+The `InkeepAIStreamCallbacksAndOptions` extends the standard (AIStreamCallbacks)[/docs/api-reference/ai-stream#AIStreamCallbacks] by including additional `metadata` in `onFinal` and adding an `onRecordsCited` callback.
+
+<OptionTable
+  options={[
+    // ... rest of `AIStreamCallbacks`
+    [
+      'onRecordsCited',
+      '(recordsCited: InkeepRecordsCitedData) => void',
+      'An optional function that is called once for every request, after the main content of a message has completed. It includes the information about the records (sources) cited in the AI chat response.',
+    ],
+    [
+      'onFinal',
+      '(completion: string, metadata: InkeepOnFinalMetadata) => Promise<void>',
+      "An optional function that is called once for every request. It's always the final callback invoked. It's passed the content of the chat response as a string and metadata as an object of type `InkeepOnFinalMetadata`.",
+    ],
+  ]}
+/>
+
+#### InkeepOnFinalMetadata
+
+Information included in `metadata` of InkeepStream's `onFinal` callback.
+
+<OptionTable
+  options={[
+    [
+      'chat_session_id',
+      'string',
+      'The Inkeep chat_session_id. This is should be included in follow-up chat requests that are part of a chat session. Used for analytics and threading chat conversations.',
+    ],
+    [
+      'records_cited.citations',
+      'InkeepCitation[]',
+      'Contains information about the citations used in the chat response body.',
+    ],
+  ]}
+/>
+
+#### InkeepCitation
+
+Citations in the message content are formatted as, for example:
+
+```
+To get started with Inkeep, first create an account [1](https://inkeep.com).
+```
+
+The `citations` array in the `onFinal` and `onRecordsCited` callbacks contain additional information about the citations used.
+
+<OptionTable
+  options={[
+    [
+      'number',
+      'number',
+      'The number of the citation as included in the message content.',
+    ],
+    [
+      'record',
+      'InkeepRecord',
+      'Information abou the record that is referenced in the citation.',
+    ],
+  ]}
+/>
+
+#### InkeepRecord
+
+The `InkeepRecord` object contains information about the citation used in the message.
+
+<OptionTable
+  options={[
+    ['url', 'string | null', 'The URL of the citation, if available.'],
+    ['title', 'string | null', 'The title of the citation, if available.'],
+    [
+      'breadcrumbs',
+      'string[] | null',
+      'The breadcrumbs of the location of the record (e.g. `["Home", "About"]`)',
+    ],
+  ]}
+/>
+
+## Example
+
+The `InkeepStream` function can be coupled with a `fetch` call to either of the Inkeep API chat endpoints:
+
+1. `POST chat_sessions/chat_results` - To **create** a chat session
+2. `POST chat_sessions/${chat_session_id}/chat_results` - To **continue** a chat session
+
+This stream can then facilitate the real-time consumption of AI outputs as they're being generated.
+
+Here's a step-by-step example of how to implement a combined endpoint that automatically handles both `create` and `continue` API endpoints:
+
+```tsx filename="app/api/chat/route.ts" showLineNumbers
+import {
+  InkeepStream,
+  InkeepOnFinalMetadata,
+  StreamingTextResponse,
+  experimental_StreamData,
+  InkeepMessage,
+} from 'ai';
+
+// The type for the request body
+interface ChatRequestBody {
+  messages: Array<{
+    role: 'user' | 'assistant';
+    content: string;
+  }>;
+  chat_session_id?: string;
+}
+
+interface ChatSessionArgs {
+  integration_id: string;
+  chat_mode?: 'turbo' | 'auto'; // default: 'auto'
+  chat_session: {
+    messages: Array<InkeepMessage>;
+  };
+  stream?: boolean;
+}
+
+interface ContinueChatInput {
+  integration_id: string;
+  chat_session_id: string;
+  message: InkeepMessage;
+  stream?: boolean;
+}
+
+export async function POST(req: Request) {
+  const chatRequestBody: ChatRequestBody = await req.json();
+  const chatSessionId = chatRequestBody.chat_session_id;
+
+  let response;
+  let body;
+  let url;
+
+  if (!chatSessionId) {
+    // new chat session
+    const args: ChatSessionArgs = {
+      integration_id: process.env.INKEEP_INTEGRATION_ID!,
+      chat_session: {
+        messages: chatRequestBody.messages,
+      },
+      stream: true,
+    };
+    body = JSON.stringify(args);
+    url = 'https://api.inkeep.com/v0/chat_sessions/chat_results';
+  } else {
+    // continue chat session
+    const args: ContinueChatInput = {
+      integration_id: process.env.INKEEP_INTEGRATION_ID!,
+      chat_session_id: chatSessionId,
+      message: chatRequestBody.messages[chatRequestBody.messages.length - 1],
+      stream: true,
+    };
+    body = JSON.stringify(args);
+    url = `https://api.inkeep.com/v0/chat_sessions/${chatSessionId}/chat_results`;
+  }
+
+  response = await fetch(url, {
+    method: 'POST',
+    headers: {
+      'Content-Type': 'application/json',
+      Authorization: `Bearer ${process.env.INKEEP_API_KEY}`,
+    },
+    body,
+  });
+
+  if (!response.ok) {
+    throw new Error(`HTTP error! status: ${response.status}`);
+  }
+
+  // used to pass custom metadata to the client
+  const data = new experimental_StreamData();
+
+  if (!response?.body) {
+    throw new Error('Response body is null');
+  }
+
+  const stream = InkeepStream(response, {
+    onRecordsCited: async recordsCited => {
+      // append the citations as a message annotation
+      data.appendMessageAnnotation({
+        recordsCited,
+      });
+    },
+    onFinal: async (complete: string, metadata?: InkeepOnFinalMetadata) => {
+      // return the chat_session_id to the client
+      if (metadata) {
+        data.append({ onFinalMetadata: metadata });
+      }
+      data.close();
+    },
+    experimental_streamData: true,
+  });
+
+  return new StreamingTextResponse(stream, {}, data);
+}
+```
+
+This example uses the [experimental_StreamData](/docs/api-reference/stream-data) and the callback methods of `InkeepStream` to attach metadata to the response.
+
+From [`useChat`](/docs/api-reference/use-chat), this is available as:
+
+```tsx filename="app/chat/page.tsx" showLineNumbers
+// ... your chat component
+
+const { messages, data } = useChat();
+
+/* For chat_session_id */
+
+// get the onFinalMetadata item from the global chat data
+const onFinalMetadataItem = data?.find(
+  item =>
+    typeof item === 'object' && item !== null && 'onFinalMetadata' in item,
+) as { onFinalMetadata: InkeepOnFinalMetadata } | undefined;
+
+// get the chat_session_id from the onFinalMetadata item
+const chatSessionId = onFinalMetadataItem?.onFinalMetadata?.chat_session_id;
+
+/* For messages[n].annotations, available independently for each message */
+
+const recordsCitedAnnotation =
+  messages &&
+  messages.length > 0 &&
+  (messages[0].annotations?.find(
+    item => typeof item === 'object' && item !== null && 'recordsCited' in item,
+  ) as { recordsCited: InkeepRecordsCitedData } | undefined);
+
+// get the citations from the recordsCited annotation
+const citations = recordsCitedAnnotation?.recordsCited?.citations;
+```
diff --git a/docs/pages/docs/guides/providers/inkeep.mdx b/docs/pages/docs/guides/providers/inkeep.mdx
new file mode 100644
index 0000000..7345ea0
--- /dev/null
+++ b/docs/pages/docs/guides/providers/inkeep.mdx
@@ -0,0 +1,362 @@
+---
+title: Inkeep
+---
+
+import { Steps, Callout } from 'nextra-theme-docs';
+
+# Inkeep
+
+Vercel AI SDK provides a set of utilities to make it easy to use [Inkeep](https://inkeep.com/)'s RAG and AI chat service to create chat experiences powered by your own content.
+
+In this guide, we'll walk through how to create a Q&A support bot powered by Inkeep.
+
+<Callout>
+  You can also use Inkeep for retrieval augmented generation (RAG) components of
+  more complex LLM applications or agents.
+</Callout>
+
+## Guide: Inkeep Chatbot
+
+<Steps>
+
+### Create a Next.js app
+
+Create a Next.js application and install `ai`, the Vercel AI SDK.
+
+```sh
+pnpm dlx create-next-app my-rag-app
+cd my-rag-app
+```
+
+### Add your Inkeep API Key to `.env`
+
+Create a `.env` file in your project root and add your Inkeep API Key:
+
+```env filename=".env"
+INKEEP_API_KEY=xxxxxx
+INKEEP_INTEGRATION_ID=xxxxxx
+```
+
+### Create Inkeep API utils
+
+In order to provide analytics and correlate multiple message exchanges into a single "chat session", the Inkeep API provides two endpoints:
+
+1. `POST chat_sessions/chat_results` - To **create** a chat session
+2. `POST chat_sessions/${chat_session_id}/chat_results` - To **continue** a chat session
+
+First, lets create two helper functions for calling these endpoints.
+
+### Create `createChatSession.ts`
+
+```ts filename="app/api/chat/createChatSession.ts" showLineNumbers
+import { InkeepMessage } from 'ai/streams';
+
+// The type for the request body
+export interface CreateChatSessionArgs {
+  integration_id: string;
+  chat_mode?: 'turbo' | 'auto'; // default: 'auto'
+  chat_session: {
+    messages: Array<InkeepMessage>;
+  };
+  stream?: boolean;
+}
+
+export async function createChatSession({
+  integration_id,
+  chat_mode,
+  chat_session,
+  stream,
+}: CreateChatSessionArgs) {
+  const body = JSON.stringify({
+    integration_id,
+    chat_mode,
+    chat_session,
+    stream,
+  });
+
+  const response = await fetch(
+    'https://api.inkeep.com/v0/chat_sessions/chat_results',
+    {
+      method: 'POST',
+      headers: {
+        'Content-Type': 'application/json',
+        Authorization: `Bearer ${process.env.INKEEP_API_KEY}`,
+      },
+      body,
+    },
+  );
+
+  if (!response.ok) {
+    throw new Error(`HTTP error! status: ${response.status}`);
+  }
+
+  return response;
+}
+```
+
+#### Create `continueChatSession.ts` helper
+
+```ts filename="app/api/chat/continueChatSession.ts" showLineNumbers
+import { InkeepMessage } from 'ai/streams';
+
+// The type for the request body
+export interface ContinueChatInput {
+  integration_id: string;
+  chat_session_id: string;
+  message: InkeepMessage;
+  stream?: boolean;
+}
+
+export async function continueChatSession({
+  integration_id,
+  chat_session_id,
+  message,
+  stream,
+}: ContinueChatInput) {
+  const body = JSON.stringify({
+    integration_id,
+    chat_session_id,
+    message,
+    stream,
+  });
+
+  const response = await fetch(
+    `https://api.inkeep.com/v0/chat_sessions/${chat_session_id}/chat_results`,
+    {
+      method: 'POST',
+      headers: {
+        'Content-Type': 'application/json',
+        Authorization: `Bearer ${process.env.INKEEP_API_KEY}`,
+      },
+      body,
+    },
+  );
+
+  if (!response.ok) {
+    throw new Error(`HTTP error! status: ${response.status}`);
+  }
+
+  return response;
+}
+```
+
+### Create a Route Handler
+
+Create a Next.js Route Handler that we'll use to call the Inkeep API for creating or continuing a chat.
+
+For this example, we'll create a route handler at `app/api/chat/route.ts` that accepts a `POST` request with a `messages` array of strings and an optional `chat_session_id`, which we'll use to decide whether to create or continue a chat.
+
+```tsx filename="app/api/chat/route.ts" showLineNumbers
+import {
+  InkeepStream,
+  InkeepOnFinalMetadata,
+  StreamingTextResponse,
+  experimental_StreamData,
+} from 'ai';
+import { continueChatSession } from './inkeep/continueChatSession';
+import { createChatSession } from './inkeep/createChatSession';
+
+interface ChatRequestBody {
+  messages: Array<{
+    role: 'user' | 'assistant';
+    content: string;
+  }>;
+  chat_session_id?: string;
+}
+
+const inkeepIntegrationId = process.env.INKEEP_INTEGRATION_ID;
+
+export async function POST(req: Request) {
+  const chatRequestBody: ChatRequestBody = await req.json();
+  const chatSessionId = chatRequestBody.chat_session_id;
+
+  let response;
+  if (!chatSessionId) {
+    // new chat session
+    response = await createChatSession({
+      integration_id: inkeepIntegrationId!,
+      chat_session: {
+        messages: chatRequestBody.messages,
+      },
+      stream: true,
+    });
+  } else {
+    // continue chat session
+    response = await continueChatSession({
+      integration_id: inkeepIntegrationId!,
+      chat_session_id: chatSessionId,
+      message: chatRequestBody.messages[chatRequestBody.messages.length - 1],
+      stream: true,
+    });
+  }
+
+  // used to pass custom metadata to the client
+  const data = new experimental_StreamData();
+
+  if (!response?.body) {
+    throw new Error('Response body is null');
+  }
+
+  const stream = InkeepStream(response, {
+    onRecordsCited: async recordsCited => {
+      // append the citations to the message annotations
+      data.appendMessageAnnotation({
+        recordsCited,
+      });
+    },
+    onFinal: async (complete: string, metadata?: InkeepOnFinalMetadata) => {
+      // return the chat_session_id to the client
+      if (metadata) {
+        data.append({ onFinalMetadata: metadata });
+      }
+      data.close();
+    },
+    experimental_streamData: true,
+  });
+
+  return new StreamingTextResponse(stream, {}, data);
+}
+```
+
+<Callout>
+  It's common to save a chat to a database. To do so, you can leverage the
+  `onFinal` callback to add your own saving logic. For example, add `await
+  saveCompletionToDatabase(complete, metadata);` prior to `data.close();`.
+</Callout>
+
+This example leverages a few utilities provided by the Vercel AI SDK:
+
+1. First, we pass the streaming `response` we receive from the Inkeep API to the
+   [`InkeepStream`](/docs/api-reference/inkeep-stream). This
+   method decodes/extracts the content of the message from Inkeep's server-side events response and then re-encodes them into a standard (ReadableStream)[https://developer.mozilla.org/docs/Web/API/ReadableStream].
+
+2. We can then pass that stream directly to the Vercel AI SDK's [`StreamingTextResponse`](/docs/api-reference/streaming-text-response).
+   This is another utility class that extends the normal Node/Edge Runtime `Response`
+   class with the default headers you probably want (hint: `'Content-Type':
+'text/plain; charset=utf-8'` is already set for you). This will provide the streamed content to the client.
+
+3. Lastly, we use the [experimental_StreamData](/docs/api-reference/stream-data) and callback methods of the `InkeepStream` to attach metadata to the response like `onFinalMetadata.chat_session_id` and `recordsCited.citations` for use by the client.
+
+### Wire up the UI
+
+Next, lets create a client component with a form that we'll use to gather the prompt from the user and then stream back the chat completion from.
+
+By default, the [`useChat`](/docs/api-reference/use-chat) hook will use the `POST` Route Handler we created above (it defaults to `/api/chat`).
+
+We will use the `data` prop to get the Inkeep `chat_session_id`, which we will include in the request body in any subsequent messages.
+
+```tsx filename="app/page.tsx" showLineNumbers
+'use client';
+
+import { useChat } from 'ai/react';
+import { useEffect, useState } from 'react';
+import { Message } from 'ai';
+import { type InkeepOnFinalMetadata } from 'ai/streams';
+import { Citations } from './Citations';
+
+export default function Chat() {
+  /**
+   * You can alternatively put the chat_session_id in search params e.g. ?chat_session_id=123 or path params like /chat/123 depending on your use case
+   */
+  const [chatSessionId, setChatSessionId] = useState<string | undefined>(
+    undefined,
+  );
+
+  const { messages, input, handleInputChange, handleSubmit, data } = useChat({
+    body: {
+      chat_session_id: chatSessionId,
+    },
+  });
+
+  // SET THE INKEEP CHAT SESSION ID FROM THE CHAT DATA
+  useEffect(() => {
+    // get the onFinalMetadata item from the global data
+    const onFinalMetadataItem = data?.find(
+      item =>
+        typeof item === 'object' && item !== null && 'onFinalMetadata' in item,
+    ) as { onFinalMetadata: InkeepOnFinalMetadata } | undefined;
+
+    // get the chat_session_id from the onFinalMetadata item
+    const chatSessionId = onFinalMetadataItem?.onFinalMetadata?.chat_session_id;
+
+    setChatSessionId(chatSessionId);
+  }, [data]);
+
+  return (
+    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
+      {messages.map(m => {
+        console.log(m);
+
+        return (
+          <div key={m.id} className="whitespace-pre-wrap">
+            <br />
+            <strong>{m.role === 'user' ? 'User: ' : 'AI: '}</strong>
+            {m.content}
+            <Citations annotations={m.annotations} />
+          </div>
+        );
+      })}
+
+      <form onSubmit={handleSubmit}>
+        <input
+          className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"
+          value={input}
+          placeholder="Say something..."
+          onChange={handleInputChange}
+        />
+      </form>
+    </div>
+  );
+}
+```
+
+#### Show Citations (optional)
+
+The Inkeep API provides information about the sources (documentation, web pages, forums, etc.) used to answer a question in a `recordsCited` message annotation.
+
+We can use this to display a list of "Citations" at the end of the main chat message content.
+
+```tsx filename="app/Citations.tsx" showLineNumbers
+import { Message } from 'ai';
+import { InkeepRecordsCitedData } from 'ai/streams';
+
+interface CitationsProps {
+  annotations: Message['annotations'];
+}
+
+export const Citations = ({ annotations }: CitationsProps) => {
+  // get the recordsCited annotation of the message
+  const recordsCitedAnnotation = annotations?.find(
+    item => typeof item === 'object' && item !== null && 'recordsCited' in item,
+  ) as { recordsCited: InkeepRecordsCitedData } | undefined;
+
+  // get the citations from the recordsCited annotation
+  const citations = recordsCitedAnnotation?.recordsCited?.citations;
+
+  return (
+    citations && (
+      <>
+        {annotations && annotations.length > 0 && (
+          <div>
+            <br />
+            {'---SOURCES USED---'}
+            <br />
+            <div>
+              {citations.map((citation, citationIndex) => (
+                <p key={citationIndex}>
+                  {citationIndex + 1}.{' '}
+                  <a target="_blank" href={citation.record.url || ''}>
+                    {citation.record.title}
+                  </a>
+                </p>
+              ))}
+            </div>
+          </div>
+        )}
+      </>
+    )
+  );
+};
+```
+
+</Steps>
diff --git a/examples/next-inkeep/.env.local.example b/examples/next-inkeep/.env.local.example
new file mode 100644
index 0000000..446eaf9
--- /dev/null
+++ b/examples/next-inkeep/.env.local.example
@@ -0,0 +1,2 @@
+INKEEP_API_KEY=xxxxxx
+INKEEP_INTEGRATION_ID=xxxxxx
\ No newline at end of file
diff --git a/examples/next-inkeep/.gitignore b/examples/next-inkeep/.gitignore
new file mode 100644
index 0000000..8f322f0
--- /dev/null
+++ b/examples/next-inkeep/.gitignore
@@ -0,0 +1,35 @@
+# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.
+
+# dependencies
+/node_modules
+/.pnp
+.pnp.js
+
+# testing
+/coverage
+
+# next.js
+/.next/
+/out/
+
+# production
+/build
+
+# misc
+.DS_Store
+*.pem
+
+# debug
+npm-debug.log*
+yarn-debug.log*
+yarn-error.log*
+
+# local env files
+.env*.local
+
+# vercel
+.vercel
+
+# typescript
+*.tsbuildinfo
+next-env.d.ts
diff --git a/examples/next-inkeep/README.md b/examples/next-inkeep/README.md
new file mode 100644
index 0000000..9dbbba3
--- /dev/null
+++ b/examples/next-inkeep/README.md
@@ -0,0 +1,41 @@
+# Vercel AI SDK, Next.js, and Inkeep Chat Example
+
+This example shows how to use the [Vercel AI SDK](https://sdk.vercel.ai/docs) with [Next.js](https://nextjs.org/) and [Inkeep's Managed AI Chat Service](https://docs.inkeep.com/claude/reference/getting-started-with-the-api) to create an LLM-powered chat bot on your content.
+
+## Deploy your own
+
+Deploy the example using [Vercel](https://vercel.com?utm_source=github&utm_medium=readme&utm_campaign=ai-sdk-example):
+
+[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fvercel%2Fai%2Ftree%2Fmain%2Fexamples%2Fnext-inkeep&env=INKEEP_API_KEY&envDescription=Inkeep_API_Key&envLink=https://console.inkeep.com/account/keys&project-name=vercel-ai-chat-inkeep&repository-name=vercel-ai-chat-inkeep)
+
+## How to use
+
+Execute [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app) with [npm](https://docs.npmjs.com/cli/init), [Yarn](https://yarnpkg.com/lang/en/docs/cli/create/), or [pnpm](https://pnpm.io) to bootstrap the example:
+
+```bash
+npx create-next-app --example https://github.com/vercel/ai/tree/main/examples/next-inkeep next-inkeep-app
+```
+
+```bash
+yarn create next-app --example https://github.com/vercel/ai/tree/main/examples/next-inkeep next-inkeep-app
+```
+
+```bash
+pnpm create next-app --example https://github.com/vercel/ai/tree/main/examples/next-inkeep next-inkeep-app
+```
+
+To run the example locally you need to:
+
+1. [Get onboarded](https://docs.inkeep.com/overview/getting-started) to Inkeep to receive an Inkeep API Key and Integration ID.
+2. Set the required Inkeep environment variables as the token value as shown [the example env file](./.env.local.example) but in a new file called `.env.local`
+3. `pnpm install` to install the required dependencies.
+4. `pnpm dev` to launch the development server.
+
+## Learn More
+
+To learn more about OpenAI, Next.js, and the Vercel AI SDK take a look at the following resources:
+
+- [Vercel AI SDK docs](https://sdk.vercel.ai/docs)
+- [Vercel AI Playground](https://play.vercel.ai)
+- [Inkeep Documentation](https://docs.inkeep.com) - learn about Inkeep features and API.
+- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
diff --git a/examples/next-inkeep/app/api/chat/inkeep-api/continueChatSession.ts b/examples/next-inkeep/app/api/chat/inkeep-api/continueChatSession.ts
new file mode 100644
index 0000000..6577005
--- /dev/null
+++ b/examples/next-inkeep/app/api/chat/inkeep-api/continueChatSession.ts
@@ -0,0 +1,41 @@
+import { InkeepMessage } from 'ai/streams';
+
+// The type for the request body
+export interface ContinueChatInput {
+  integration_id: string;
+  chat_session_id: string;
+  message: InkeepMessage;
+  stream?: boolean;
+}
+
+export async function continueChatSession({
+  integration_id,
+  chat_session_id,
+  message,
+  stream,
+}: ContinueChatInput) {
+  const body = JSON.stringify({
+    integration_id,
+    chat_session_id,
+    message,
+    stream,
+  });
+
+  const response = await fetch(
+    `https://api.inkeep.com/v0/chat_sessions/${chat_session_id}/chat_results`,
+    {
+      method: 'POST',
+      headers: {
+        'Content-Type': 'application/json',
+        Authorization: `Bearer ${process.env.INKEEP_API_KEY}`,
+      },
+      body,
+    },
+  );
+
+  if (!response.ok) {
+    throw new Error(`HTTP error! status: ${response.status}`);
+  }
+
+  return response;
+}
diff --git a/examples/next-inkeep/app/api/chat/inkeep-api/createChatSession.ts b/examples/next-inkeep/app/api/chat/inkeep-api/createChatSession.ts
new file mode 100644
index 0000000..627255f
--- /dev/null
+++ b/examples/next-inkeep/app/api/chat/inkeep-api/createChatSession.ts
@@ -0,0 +1,43 @@
+import { InkeepMessage } from 'ai/streams';
+
+// The type for the request body
+export interface CreateChatSessionArgs {
+  integration_id: string;
+  chat_mode?: 'turbo' | 'auto'; // default: 'auto'
+  chat_session: {
+    messages: Array<InkeepMessage>;
+  };
+  stream?: boolean;
+}
+
+export async function createChatSession({
+  integration_id,
+  chat_mode,
+  chat_session,
+  stream,
+}: CreateChatSessionArgs) {
+  const body = JSON.stringify({
+    integration_id,
+    chat_mode,
+    chat_session,
+    stream,
+  });
+
+  const response = await fetch(
+    'https://api.inkeep.com/v0/chat_sessions/chat_results',
+    {
+      method: 'POST',
+      headers: {
+        'Content-Type': 'application/json',
+        Authorization: `Bearer ${process.env.INKEEP_API_KEY}`,
+      },
+      body,
+    },
+  );
+
+  if (!response.ok) {
+    throw new Error(`HTTP error! status: ${response.status}`);
+  }
+
+  return response;
+}
diff --git a/examples/next-inkeep/app/api/chat/route.ts b/examples/next-inkeep/app/api/chat/route.ts
new file mode 100644
index 0000000..732df82
--- /dev/null
+++ b/examples/next-inkeep/app/api/chat/route.ts
@@ -0,0 +1,69 @@
+import {
+  InkeepStream,
+  InkeepOnFinalMetadata,
+  StreamingTextResponse,
+  experimental_StreamData,
+} from 'ai';
+import { continueChatSession } from './inkeep-api/continueChatSession';
+import { createChatSession } from './inkeep-api/createChatSession';
+
+interface ChatRequestBody {
+  messages: Array<{
+    role: 'user' | 'assistant';
+    content: string;
+  }>;
+  chat_session_id?: string;
+}
+
+const inkeepIntegrationId = process.env.INKEEP_INTEGRATION_ID;
+
+export async function POST(req: Request) {
+  const chatRequestBody: ChatRequestBody = await req.json();
+  const chatSessionId = chatRequestBody.chat_session_id;
+
+  let response;
+  if (!chatSessionId) {
+    // new chat session
+    response = await createChatSession({
+      integration_id: inkeepIntegrationId!,
+      chat_session: {
+        messages: chatRequestBody.messages,
+      },
+      stream: true,
+    });
+  } else {
+    // continue chat session
+    response = await continueChatSession({
+      integration_id: inkeepIntegrationId!,
+      chat_session_id: chatSessionId,
+      message: chatRequestBody.messages[chatRequestBody.messages.length - 1],
+      stream: true,
+    });
+  }
+
+  // used to pass custom metadata to the client
+  const data = new experimental_StreamData();
+
+  if (!response?.body) {
+    throw new Error('Response body is null');
+  }
+
+  const stream = InkeepStream(response, {
+    onRecordsCited: async recordsCited => {
+      // append the citations to the message annotations
+      data.appendMessageAnnotation({
+        recordsCited,
+      });
+    },
+    onFinal: async (complete: string, metadata?: InkeepOnFinalMetadata) => {
+      // return the chat_session_id to the client
+      if (metadata) {
+        data.append({ onFinalMetadata: metadata });
+      }
+      data.close();
+    },
+    experimental_streamData: true,
+  });
+
+  return new StreamingTextResponse(stream, {}, data);
+}
diff --git a/examples/next-inkeep/app/globals.css b/examples/next-inkeep/app/globals.css
new file mode 100644
index 0000000..b5c61c9
--- /dev/null
+++ b/examples/next-inkeep/app/globals.css
@@ -0,0 +1,3 @@
+@tailwind base;
+@tailwind components;
+@tailwind utilities;
diff --git a/examples/next-inkeep/app/layout.tsx b/examples/next-inkeep/app/layout.tsx
new file mode 100644
index 0000000..34624c5
--- /dev/null
+++ b/examples/next-inkeep/app/layout.tsx
@@ -0,0 +1,21 @@
+import './globals.css';
+import { Inter } from 'next/font/google';
+
+const inter = Inter({ subsets: ['latin'] });
+
+export const metadata = {
+  title: 'Create Next App',
+  description: 'Generated by create next app',
+};
+
+export default function RootLayout({
+  children,
+}: {
+  children: React.ReactNode;
+}) {
+  return (
+    <html lang="en">
+      <body className={inter.className}>{children}</body>
+    </html>
+  );
+}
diff --git a/examples/next-inkeep/app/page.tsx b/examples/next-inkeep/app/page.tsx
new file mode 100644
index 0000000..3983f97
--- /dev/null
+++ b/examples/next-inkeep/app/page.tsx
@@ -0,0 +1,100 @@
+'use client';
+
+import { useChat } from 'ai/react';
+import { useEffect, useState } from 'react';
+import { InkeepRecordsCitedData, Message } from 'ai';
+import { InkeepOnFinalMetadata } from 'ai/streams';
+
+export default function Chat() {
+  /**
+   * you can also put the chat session id in search params e.g. ?chat_session_id=123
+   * or path params like /chat/123 depending on your use case
+   */
+  const [chatSessionId, setChatSessionId] = useState<string | undefined>(
+    undefined,
+  );
+
+  const { messages, input, handleInputChange, handleSubmit, data } = useChat({
+    body: {
+      chat_session_id: chatSessionId,
+    },
+  });
+
+  // SET THE INKEEP CHAT SESSION ID FROM THE CHAT DATA
+  useEffect(() => {
+    // get the onFinalMetadata item from the global data
+    const onFinalMetadataItem = data?.find(
+      item =>
+        typeof item === 'object' && item !== null && 'onFinalMetadata' in item,
+    ) as { onFinalMetadata: InkeepOnFinalMetadata } | undefined;
+
+    // get the chat_session_id from the onFinalMetadata item
+    const chatSessionId = onFinalMetadataItem?.onFinalMetadata?.chat_session_id;
+
+    setChatSessionId(chatSessionId);
+  }, [data]);
+
+  return (
+    <div className="flex flex-col w-full max-w-md py-24 mx-auto stretch">
+      {messages.map(m => {
+        console.log(m);
+
+        return (
+          <div key={m.id} className="whitespace-pre-wrap">
+            <br />
+            <strong>{m.role === 'user' ? 'User: ' : 'AI: '}</strong>
+            {m.content}
+            <Citations annotations={m.annotations} />
+          </div>
+        );
+      })}
+
+      <form onSubmit={handleSubmit}>
+        <input
+          className="fixed bottom-0 w-full max-w-md p-2 mb-8 border border-gray-300 rounded shadow-xl"
+          value={input}
+          placeholder="Say something..."
+          onChange={handleInputChange}
+        />
+      </form>
+    </div>
+  );
+}
+
+interface CitationsProps {
+  annotations: Message['annotations'];
+}
+
+const Citations = ({ annotations }: CitationsProps) => {
+  // get the recordsCited annotation of the message
+  const recordsCitedAnnotation = annotations?.find(
+    item => typeof item === 'object' && item !== null && 'recordsCited' in item,
+  ) as { recordsCited: InkeepRecordsCitedData } | undefined;
+
+  // get the citations from the recordsCited annotation
+  const citations = recordsCitedAnnotation?.recordsCited?.citations;
+
+  return (
+    citations && (
+      <>
+        {annotations && annotations.length > 0 && (
+          <div>
+            <br />
+            {'---SOURCES USED---'}
+            <br />
+            <div>
+              {citations.map((citation, citationIndex) => (
+                <p key={citationIndex}>
+                  {citationIndex + 1}.{' '}
+                  <a target="_blank" href={citation.record.url || ''}>
+                    {citation.record.title}
+                  </a>
+                </p>
+              ))}
+            </div>
+          </div>
+        )}
+      </>
+    )
+  );
+};
diff --git a/examples/next-inkeep/next.config.js b/examples/next-inkeep/next.config.js
new file mode 100644
index 0000000..658404a
--- /dev/null
+++ b/examples/next-inkeep/next.config.js
@@ -0,0 +1,4 @@
+/** @type {import('next').NextConfig} */
+const nextConfig = {};
+
+module.exports = nextConfig;
diff --git a/examples/next-inkeep/package.json b/examples/next-inkeep/package.json
new file mode 100644
index 0000000..8a56635
--- /dev/null
+++ b/examples/next-inkeep/package.json
@@ -0,0 +1,30 @@
+{
+  "name": "next-inkeep",
+  "version": "0.0.0",
+  "private": true,
+  "scripts": {
+    "dev": "next dev",
+    "build": "next build",
+    "start": "next start",
+    "lint": "next lint"
+  },
+  "dependencies": {
+    "ai": "2.2.29",
+    "eventsource-parser": "1.1.1",
+    "next": "14.0.3",
+    "react": "18.2.0",
+    "react-dom": "^18.2.0",
+    "zod": "^3.22.4"
+  },
+  "devDependencies": {
+    "@types/node": "^17.0.12",
+    "@types/react": "18.2.8",
+    "@types/react-dom": "18.2.4",
+    "autoprefixer": "^10.4.14",
+    "eslint": "^7.32.0",
+    "eslint-config-next": "13.4.12",
+    "postcss": "^8.4.23",
+    "tailwindcss": "^3.3.2",
+    "typescript": "5.1.3"
+  }
+}
diff --git a/examples/next-inkeep/postcss.config.js b/examples/next-inkeep/postcss.config.js
new file mode 100644
index 0000000..12a703d
--- /dev/null
+++ b/examples/next-inkeep/postcss.config.js
@@ -0,0 +1,6 @@
+module.exports = {
+  plugins: {
+    tailwindcss: {},
+    autoprefixer: {},
+  },
+};
diff --git a/examples/next-inkeep/tailwind.config.js b/examples/next-inkeep/tailwind.config.js
new file mode 100644
index 0000000..db68cff
--- /dev/null
+++ b/examples/next-inkeep/tailwind.config.js
@@ -0,0 +1,18 @@
+/** @type {import('tailwindcss').Config} */
+module.exports = {
+  content: [
+    './pages/**/*.{js,ts,jsx,tsx,mdx}',
+    './components/**/*.{js,ts,jsx,tsx,mdx}',
+    './app/**/*.{js,ts,jsx,tsx,mdx}',
+  ],
+  theme: {
+    extend: {
+      backgroundImage: {
+        'gradient-radial': 'radial-gradient(var(--tw-gradient-stops))',
+        'gradient-conic':
+          'conic-gradient(from 180deg at 50% 50%, var(--tw-gradient-stops))',
+      },
+    },
+  },
+  plugins: [],
+};
diff --git a/examples/next-inkeep/tsconfig.json b/examples/next-inkeep/tsconfig.json
new file mode 100644
index 0000000..1b1ff06
--- /dev/null
+++ b/examples/next-inkeep/tsconfig.json
@@ -0,0 +1,28 @@
+{
+  "compilerOptions": {
+    "target": "es5",
+    "lib": ["dom", "dom.iterable", "esnext"],
+    "allowJs": true,
+    "skipLibCheck": true,
+    "strict": true,
+    "forceConsistentCasingInFileNames": true,
+    "noEmit": true,
+    "esModuleInterop": true,
+    "module": "esnext",
+    "moduleResolution": "node",
+    "resolveJsonModule": true,
+    "isolatedModules": true,
+    "jsx": "preserve",
+    "incremental": true,
+    "plugins": [
+      {
+        "name": "next"
+      }
+    ],
+    "paths": {
+      "@/*": ["./*"]
+    }
+  },
+  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
+  "exclude": ["node_modules"]
+}
\ No newline at end of file
diff --git a/package.json b/package.json
index 55486e4..ef211f6 100644
--- a/package.json
+++ b/package.json
@@ -4,7 +4,7 @@
     "build": "turbo build",
     "changeset": "changeset",
     "clean": "turbo clean",
-    "dev": "turbo dev --no-cache  --concurrency 16 --continue",
+    "dev": "turbo dev --no-cache  --concurrency 100 --continue",
     "lint": "turbo lint",
     "prepare": "husky install",
     "prettier-check": "prettier --check \"**/*.{js,ts,tsx,md,mdx}\"",
diff --git a/packages/core/streams/index.ts b/packages/core/streams/index.ts
index 8995395..4477a2a 100644
--- a/packages/core/streams/index.ts
+++ b/packages/core/streams/index.ts
@@ -1,3 +1,13 @@
+export * from './ai-stream';
+export * from './aws-bedrock-stream';
+export * from './openai-stream';
+export * from './streaming-text-response';
+export * from './huggingface-stream';
+export * from './cohere-stream';
+export * from './anthropic-stream';
+export * from './inkeep-stream';
+export * from './langchain-stream';
+export * from './replicate-stream';
 export * from '../shared/types';
 export * from '../shared/utils';
 export * from './ai-stream';
@@ -7,6 +17,7 @@ export * from './aws-bedrock-stream';
 export * from './cohere-stream';
 export * from './google-generative-ai-stream';
 export * from './huggingface-stream';
+export * from './inkeep-stream';
 export * from './langchain-stream';
 export * from './openai-stream';
 export * from './replicate-stream';
diff --git a/packages/core/streams/inkeep-stream.test.ts b/packages/core/streams/inkeep-stream.test.ts
new file mode 100644
index 0000000..acf0afd
--- /dev/null
+++ b/packages/core/streams/inkeep-stream.test.ts
@@ -0,0 +1,95 @@
+import {
+  InkeepStream,
+  StreamingTextResponse,
+  experimental_StreamData,
+} from '.';
+import { InkeepContentChunks } from '../tests/snapshots/inkeep';
+import { readAllChunks } from '../tests/utils/mock-client';
+import { DEFAULT_TEST_URL, createMockServer } from '../tests/utils/mock-server';
+
+const server = createMockServer([
+  {
+    url: DEFAULT_TEST_URL,
+    chunks: InkeepContentChunks,
+    formatChunk: chunk =>
+      `event: chat_result\ndata: ${JSON.stringify(chunk)}\n\n`,
+  },
+]);
+
+describe('InkeepStream', () => {
+  beforeAll(() => {
+    server.listen();
+  });
+
+  afterEach(() => {
+    server.resetHandlers();
+  });
+
+  afterAll(() => {
+    server.close();
+  });
+
+  it('should be able to parse SSE and receive the streamed response', async () => {
+    const response = await fetch(DEFAULT_TEST_URL);
+
+    const stream = InkeepStream(response);
+
+    const responseStream = new StreamingTextResponse(stream);
+
+    expect(await readAllChunks(responseStream)).toEqual([
+      ' Hello',
+      ',',
+      ' world',
+      '.',
+    ]);
+  });
+
+  describe('StreamData protocol', () => {
+    it('should send text', async () => {
+      const data = new experimental_StreamData();
+
+      const response = await fetch(DEFAULT_TEST_URL);
+
+      const stream = InkeepStream(response, {
+        onFinal() {
+          data.close();
+        },
+        experimental_streamData: true,
+      });
+
+      const responseStream = new StreamingTextResponse(stream, {}, data);
+
+      expect(await readAllChunks(responseStream)).toEqual([
+        '0:" Hello"\n',
+        '0:","\n',
+        '0:" world"\n',
+        '0:"."\n',
+      ]);
+    });
+
+    it('should send text and data', async () => {
+      const data = new experimental_StreamData();
+
+      data.append({ t1: 'v1' });
+
+      const response = await fetch(DEFAULT_TEST_URL);
+
+      const stream = InkeepStream(response, {
+        onFinal() {
+          data.close();
+        },
+        experimental_streamData: true,
+      });
+
+      const responseStream = new StreamingTextResponse(stream, {}, data);
+
+      expect(await readAllChunks(responseStream)).toEqual([
+        '2:[{"t1":"v1"}]\n',
+        '0:" Hello"\n',
+        '0:","\n',
+        '0:" world"\n',
+        '0:"."\n',
+      ]);
+    });
+  });
+});
diff --git a/packages/core/streams/inkeep-stream.ts b/packages/core/streams/inkeep-stream.ts
new file mode 100644
index 0000000..3bf4c14
--- /dev/null
+++ b/packages/core/streams/inkeep-stream.ts
@@ -0,0 +1,99 @@
+// packages/core/streams/inkeep-stream.ts
+import {
+  AIStream,
+  type AIStreamCallbacksAndOptions,
+  AIStreamParser,
+} from './ai-stream';
+import { createStreamDataTransformer } from './stream-data';
+
+export type InkeepMessage = {
+  role: 'user' | 'assistant';
+  content: string;
+  [key: string]: any;
+};
+
+export type InkeepMessageChunkData = {
+  chat_session_id: string;
+  content_chunk: string;
+  finish_reason?: string | null;
+};
+
+export type InkeepOnFinalMetadata = {
+  chat_session_id: string;
+  records_cited: InkeepRecordsCitedData;
+};
+
+export type InkeepRecord = {
+  url?: string | null;
+  title?: string | null;
+  breadcrumbs?: string[] | null;
+};
+
+export type InkeepCitation = {
+  number: number;
+  record: InkeepRecord;
+};
+
+export type InkeepRecordsCitedData = {
+  citations: InkeepCitation[];
+};
+
+export type InkeepChatResultCallbacks = {
+  onFinal?: (
+    completion: string,
+    metadata?: InkeepOnFinalMetadata,
+  ) => Promise<void> | void;
+  onRecordsCited?: (recordsCited: InkeepRecordsCitedData) => void;
+};
+
+export type InkeepAIStreamCallbacksAndOptions = AIStreamCallbacksAndOptions &
+  InkeepChatResultCallbacks;
+
+export function InkeepStream(
+  res: Response,
+  callbacks?: InkeepAIStreamCallbacksAndOptions,
+): ReadableStream {
+  if (!res.body) {
+    throw new Error('Response body is null');
+  }
+
+  let chat_session_id = '';
+  let records_cited: InkeepRecordsCitedData;
+
+  const inkeepEventParser: AIStreamParser = (data: string, options) => {
+    const { event } = options;
+
+    let inkeepContentChunk: InkeepMessageChunkData;
+
+    if (event === 'records_cited') {
+      const recordsCited = JSON.parse(data) as InkeepRecordsCitedData;
+      records_cited = JSON.parse(data) as InkeepRecordsCitedData;
+      callbacks?.onRecordsCited?.(recordsCited);
+    }
+
+    if (event === 'message_chunk') {
+      inkeepContentChunk = JSON.parse(data) as InkeepMessageChunkData;
+      chat_session_id = inkeepContentChunk.chat_session_id;
+      return inkeepContentChunk.content_chunk;
+    }
+    return;
+  };
+
+  let { onRecordsCited, ...passThroughCallbacks } = callbacks || {};
+
+  // extend onFinal callback with Inkeep specific metadata
+  passThroughCallbacks = {
+    ...passThroughCallbacks,
+    onFinal: completion => {
+      const inkeepOnFinalMetadata: InkeepOnFinalMetadata = {
+        chat_session_id,
+        records_cited,
+      };
+      callbacks?.onFinal?.(completion, inkeepOnFinalMetadata);
+    },
+  };
+
+  return AIStream(res, inkeepEventParser, passThroughCallbacks).pipeThrough(
+    createStreamDataTransformer(passThroughCallbacks?.experimental_streamData),
+  );
+}
diff --git a/packages/core/tests/snapshots/inkeep.ts b/packages/core/tests/snapshots/inkeep.ts
new file mode 100644
index 0000000..38bb73e
--- /dev/null
+++ b/packages/core/tests/snapshots/inkeep.ts
@@ -0,0 +1,27 @@
+export const InkeepContentChunks = [
+  {
+    chat_session_id: 'bd5d957d-1a4d-47f2-adb4-2472d828d597',
+    content_chunk: ' Hello',
+    final_response: null,
+  },
+  {
+    chat_session_id: 'bd5d957d-1a4d-47f2-adb4-2472d828d597',
+    content_chunk: ',',
+    final_response: null,
+  },
+  {
+    chat_session_id: 'bd5d957d-1a4d-47f2-adb4-2472d828d597',
+    content_chunk: ' world',
+    final_response: null,
+  },
+  {
+    chat_session_id: 'bd5d957d-1a4d-47f2-adb4-2472d828d597',
+    content_chunk: '.',
+    final_response: null,
+  },
+  {
+    chat_session_id: 'bd5d957d-1a4d-47f2-adb4-2472d828d597',
+    content_chunk: '',
+    final_response: 'stop',
+  },
+];
diff --git a/pnpm-lock.yaml b/pnpm-lock.yaml
index e70e5dc..4eafe96 100644
--- a/pnpm-lock.yaml
+++ b/pnpm-lock.yaml
@@ -309,6 +309,55 @@ importers:
         specifier: 5.1.3
         version: 5.1.3
 
+  examples/next-inkeep:
+    dependencies:
+      ai:
+        specifier: 2.2.29
+        version: 2.2.29(react@18.2.0)(solid-js@1.8.7)(svelte@4.2.3)(vue@3.3.8)
+      eventsource-parser:
+        specifier: 1.1.1
+        version: 1.1.1
+      next:
+        specifier: 14.0.3
+        version: 14.0.3(react-dom@18.2.0)(react@18.2.0)
+      react:
+        specifier: 18.2.0
+        version: 18.2.0
+      react-dom:
+        specifier: ^18.2.0
+        version: 18.2.0(react@18.2.0)
+      zod:
+        specifier: ^3.22.4
+        version: 3.22.4
+    devDependencies:
+      '@types/node':
+        specifier: ^17.0.12
+        version: 17.0.45
+      '@types/react':
+        specifier: 18.2.8
+        version: 18.2.8
+      '@types/react-dom':
+        specifier: 18.2.4
+        version: 18.2.4
+      autoprefixer:
+        specifier: ^10.4.14
+        version: 10.4.16(postcss@8.4.31)
+      eslint:
+        specifier: ^7.32.0
+        version: 7.32.0
+      eslint-config-next:
+        specifier: 13.4.12
+        version: 13.4.12(eslint@7.32.0)(typescript@5.1.3)
+      postcss:
+        specifier: ^8.4.23
+        version: 8.4.31
+      tailwindcss:
+        specifier: ^3.3.2
+        version: 3.3.5
+      typescript:
+        specifier: 5.1.3
+        version: 5.1.3
+
   examples/next-langchain:
     dependencies:
       ai:
@@ -6046,6 +6095,37 @@ packages:
       vue: 3.3.8(typescript@5.1.3)
     dev: false
 
+  /ai@2.2.29(react@18.2.0)(solid-js@1.8.7)(svelte@4.2.3)(vue@3.3.8):
+    resolution: {integrity: sha512-/zzSTTKF5LxMGQuNVUnNjs7X6PWYfb6M88Zn74gCUnM3KCYgh0CiAWhLyhKP6UtK0H5mHSmXgt0ZkZYUecRp0w==}
+    engines: {node: '>=14.6'}
+    peerDependencies:
+      react: ^18.2.0
+      solid-js: ^1.7.7
+      svelte: ^3.0.0 || ^4.0.0
+      vue: ^3.3.4
+    peerDependenciesMeta:
+      react:
+        optional: true
+      solid-js:
+        optional: true
+      svelte:
+        optional: true
+      vue:
+        optional: true
+    dependencies:
+      eventsource-parser: 1.0.0
+      nanoid: 3.3.6
+      react: 18.2.0
+      solid-js: 1.8.7
+      solid-swr-store: 0.10.7(solid-js@1.8.7)(swr-store@0.10.6)
+      sswr: 2.0.0(svelte@4.2.3)
+      svelte: 4.2.3
+      swr: 2.2.0(react@18.2.0)
+      swr-store: 0.10.6
+      swrv: 1.0.4(vue@3.3.8)
+      vue: 3.3.8(typescript@5.1.3)
+    dev: false
+
   /ajv-keywords@3.5.2(ajv@6.12.6):
     resolution: {integrity: sha512-5p6WTN0DdTGVQk6VjcEju19IgaHudalcfabD7yhDGeA6bcQnmL+CpveLJq/3hvfwd1aof6L386Ougkx6RfyMIQ==}
     peerDependencies:
@@ -8466,6 +8546,11 @@ packages:
     engines: {node: '>=14.18'}
     dev: false
 
+  /eventsource-parser@1.1.1:
+    resolution: {integrity: sha512-3Ej2iLj6ZnX+5CMxqyUb8syl9yVZwcwm8IIMrOJlF7I51zxOOrRlU3zxSb/6hFbl03ts1ZxHAGJdWLZOLyKG7w==}
+    engines: {node: '>=14.18'}
+    dev: false
+
   /execa@5.1.1:
     resolution: {integrity: sha512-8uSpZZocAZRBAPIEINJj3Lo9HyGitllczc27Eh5YYojjMFMn8yHMDMaUHE2Jqfq05D/wucwI4JGURyXt1vchyg==}
     engines: {node: '>=10'}
diff --git a/turbo.json b/turbo.json
index 07f4016..76df98a 100644
--- a/turbo.json
+++ b/turbo.json
@@ -17,7 +17,9 @@
         "PERPLEXITY_API_KEY",
         "REPLICATE_API_KEY",
         "NODE_ENV",
-        "ASSISTANT_ID"
+        "ASSISTANT_ID",
+        "INKEEP_API_KEY",
+        "INKEEP_INTEGRATION_ID"
       ],
       "outputs": ["dist/**", ".next/**", "!.next/cache/**"]
     },
